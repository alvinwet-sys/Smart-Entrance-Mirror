import os
import cv2
import numpy as np
import json
import time
import uuid
import asyncio
import threading
from concurrent.futures import ThreadPoolExecutor
import onnxruntime as ort
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class FaceTracker:
    """äººè„¸è·Ÿè¸ªå™¨ï¼Œç”¨äºç»´æŠ¤äººè„¸è½¨è¿¹ID"""
    
    def __init__(self, max_disappeared=5, max_distance=50):
        self.next_object_id = 0
        self.objects = {}
        self.disappeared = {}
        self.max_disappeared = max_disappeared
        self.max_distance = max_distance
        
    def register(self, centroid, features):
        """æ³¨å†Œæ–°çš„äººè„¸è½¨è¿¹"""
        if features is None:
            return None
            
        object_id = self.next_object_id
        self.objects[object_id] = {
            'centroids': [centroid],
            'features': [features],
            'last_seen': time.time()
        }
        self.disappeared[object_id] = 0
        self.next_object_id += 1
        return object_id
    
    def deregister(self, object_id):
        """æ³¨é”€äººè„¸è½¨è¿¹"""
        if object_id in self.objects:
            del self.objects[object_id]
        if object_id in self.disappeared:
            del self.disappeared[object_id]
    
    def update(self, detections):
        """æ›´æ–°è·Ÿè¸ªçŠ¶æ€"""
        if len(detections) == 0:
            for object_id in list(self.disappeared.keys()):
                self.disappeared[object_id] += 1
                if self.disappeared[object_id] > self.max_disappeared:
                    self.deregister(object_id)
            return self.objects
        
        # è®¡ç®—å½“å‰æ£€æµ‹æ¡†çš„ä¸­å¿ƒç‚¹
        input_centroids = []
        input_features = []
        for detection in detections:
            x1, y1, x2, y2 = detection['bbox']
            centroid = ((x1 + x2) / 2, (y1 + y2) / 2)
            input_centroids.append(centroid)
            input_features.append(detection.get('features', None))
        
        if len(self.objects) == 0:
            for i in range(len(detections)):
                if input_features[i] is not None:
                    self.register(input_centroids[i], input_features[i])
            return self.objects
        
        # åŒ¹é…ç°æœ‰è½¨è¿¹
        object_ids = list(self.objects.keys())
        object_centroids = []
        for object_id in object_ids:
            centroids = self.objects[object_id]['centroids']
            object_centroids.append(centroids[-1])
        
        # è®¡ç®—è·ç¦»çŸ©é˜µ
        D = np.zeros((len(object_ids), len(input_centroids)))
        for i in range(len(object_ids)):
            for j in range(len(input_centroids)):
                dx = object_centroids[i][0] - input_centroids[j][0]
                dy = object_centroids[i][1] - input_centroids[j][1]
                D[i, j] = np.sqrt(dx*dx + dy*dy)
        
        # åŒ¹é…è½¨è¿¹
        used_rows = set()
        used_cols = set()
        
        for i in range(len(object_ids)):
            if i in used_rows:
                continue
                
            min_val = np.inf
            min_col = -1
            
            for j in range(len(input_centroids)):
                if j in used_cols:
                    continue
                if D[i, j] < min_val:
                    min_val = D[i, j]
                    min_col = j
            
            if min_val < self.max_distance:
                object_id = object_ids[i]
                self.objects[object_id]['centroids'].append(input_centroids[min_col])
                if input_features[min_col] is not None:
                    self.objects[object_id]['features'].append(input_features[min_col])
                self.objects[object_id]['last_seen'] = time.time()
                self.disappeared[object_id] = 0
                used_rows.add(i)
                used_cols.add(min_col)
        
        # å¤„ç†æœªåŒ¹é…çš„è½¨è¿¹å’Œæ£€æµ‹
        for i in range(len(object_ids)):
            if i not in used_rows:
                object_id = object_ids[i]
                self.disappeared[object_id] += 1
                if self.disappeared[object_id] > self.max_disappeared:
                    self.deregister(object_id)
        
        for j in range(len(input_centroids)):
            if j not in used_cols and input_features[j] is not None:
                self.register(input_centroids[j], input_features[j])
        
        return self.objects

class BuffaloFaceRecognizer:
    """ä½¿ç”¨Buffalo_Mæ¨¡å‹çš„äººè„¸è¯†åˆ«å™¨"""
    
    def __init__(self, model_path, gallery_dir, gpu_id=0):
        # è®¾ç½®ONNX Runtimeæ‰§è¡Œæä¾›è€…
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if gpu_id >= 0 else ['CPUExecutionProvider']
        
        # æ‰“å°è¯¦ç»†çš„è·¯å¾„ä¿¡æ¯
        print(f"åˆå§‹åŒ–BuffaloFaceRecognizer:")
        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"æ³¨å†Œåº“è·¯å¾„: {gallery_dir}")
        
        # åŠ è½½Buffalo_Mæ¨¡å‹
        self.recognizer = ort.InferenceSession(model_path, providers=providers)
        print(f"å·²åŠ è½½Buffalo_Mæ¨¡å‹: {model_path}")
        
        # æ‰“å°æ¨¡å‹è¾“å…¥è¾“å‡ºä¿¡æ¯
        print("æ¨¡å‹è¾“å…¥ä¿¡æ¯:")
        for input_info in self.recognizer.get_inputs():
            print(f"  - åç§°: {input_info.name}, å½¢çŠ¶: {input_info.shape}, ç±»å‹: {input_info.type}")
        
        print("æ¨¡å‹è¾“å‡ºä¿¡æ¯:")
        for output_info in self.recognizer.get_outputs():
            print(f"  - åç§°: {output_info.name}, å½¢çŠ¶: {output_info.shape}, ç±»å‹: {output_info.type}")
        
        # åŠ è½½äººè„¸æ£€æµ‹å™¨
        self.face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        
        # åŠ è½½æ³¨å†Œåº“
        self.gallery_dir = gallery_dir
        if not self.load_gallery():
            raise Exception("æ— æ³•åŠ è½½æ³¨å†Œåº“")
        
        self.threshold = 0.5  # è°ƒæ•´é˜ˆå€¼
        self.tracker = FaceTracker()
        self.min_face_size = 30
        
        print("Buffalo_Mäººè„¸è¯†åˆ«å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_gallery(self):
        """åŠ è½½æ³¨å†Œåº“"""
        # é¦–å…ˆå°è¯•åŠ è½½Buffalo_Mæ³¨å†Œåº“
        gallery_file = Path(self.gallery_dir) / "gallery_data_buffalo.json"
        
        if not gallery_file.exists():
            # å¦‚æœBuffalo_Mæ³¨å†Œåº“ä¸å­˜åœ¨ï¼Œå°è¯•åŠ è½½å…¶ä»–æ³¨å†Œåº“
            gallery_file = Path(self.gallery_dir) / "gallery_data.json"
            print(f"ä½¿ç”¨æ—§ç‰ˆæ³¨å†Œåº“: {gallery_file}")
        
        if not gallery_file.exists():
            print(f"æ³¨å†Œåº“æ–‡ä»¶ä¸å­˜åœ¨: {gallery_file}")
            return False
        
        try:
            with open(gallery_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.gallery_embeddings = data.get('gallery_embeddings', {})
            print(f"æ³¨å†Œåº“åŠ è½½æˆåŠŸ: {len(self.gallery_embeddings)} ä¸ªèº«ä»½")
            
            # è½¬æ¢åµŒå…¥å‘é‡ä¸ºnumpyæ•°ç»„
            for subject_id, embeddings in self.gallery_embeddings.items():
                self.gallery_embeddings[subject_id] = [np.array(emb) for emb in embeddings]
            
            return True
            
        except Exception as e:
            print(f"åŠ è½½æ³¨å†Œåº“å¤±è´¥: {e}")
            return False
    
    def detect_faces(self, image):
        """æ£€æµ‹å›¾åƒä¸­çš„äººè„¸"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            faces = self.face_cascade.detectMultiScale(
                gray, 
                scaleFactor=1.1, 
                minNeighbors=5, 
                minSize=(self.min_face_size, self.min_face_size)
            )
            
            results = []
            for (x, y, w, h) in faces:
                x1, y1, x2, y2 = x, y, x + w, y + h
                
                # ç”Ÿæˆç®€å•çš„å…³é”®ç‚¹
                landmarks = [
                    [x1 + w//3, y1 + h//3],
                    [x1 + 2*w//3, y1 + h//3],
                    [x1 + w//2, y1 + h//2],
                    [x1 + w//3, y1 + 2*h//3],
                    [x1 + 2*w//3, y1 + 2*h//3]
                ]
                
                results.append({
                    'bbox': [x1, y1, x2, y2],
                    'landmarks': np.array(landmarks, dtype=np.float32),
                    'score': 0.9
                })
            
            return results
            
        except Exception as e:
            print(f"äººè„¸æ£€æµ‹å¤±è´¥: {e}")
            return []
    
    def extract_feature(self, image, bbox):
        """ä»äººè„¸å›¾åƒä¸­æå–ç‰¹å¾å‘é‡"""
        try:
            x1, y1, x2, y2 = bbox
            
            # è£å‰ªäººè„¸åŒºåŸŸ
            face_roi = image[y1:y2, x1:x2]
            if face_roi.size == 0:
                return None
            
            # è°ƒæ•´å¤§å°ä¸ºæ¨¡å‹æœŸæœ›çš„è¾“å…¥å°ºå¯¸
            aligned_face = cv2.resize(face_roi, (112, 112))
            
            # ä½¿ç”¨Buffalo_Mæ¨¡å‹çš„é¢„å¤„ç†æ–¹å¼
            aligned_face = cv2.cvtColor(aligned_face, cv2.COLOR_BGR2RGB)
            aligned_face = aligned_face.astype(np.float32)
            aligned_face = aligned_face / 255.0  # å½’ä¸€åŒ–åˆ°[0,1]
            aligned_face = (aligned_face - 0.5) / 0.5  # å½’ä¸€åŒ–åˆ°[-1,1]
            
            # è°ƒæ•´ç»´åº¦
            aligned_face = np.transpose(aligned_face, (2, 0, 1))
            aligned_face = np.expand_dims(aligned_face, axis=0)
            
            # æå–ç‰¹å¾
            feature = self.recognizer.run(
                None, 
                {self.recognizer.get_inputs()[0].name: aligned_face}
            )[0][0]
            
            # å½’ä¸€åŒ–ç‰¹å¾å‘é‡
            feature_norm = np.linalg.norm(feature)
            if feature_norm == 0:
                return None
                
            feature = feature / feature_norm
            
            return feature
            
        except Exception as e:
            print(f"ç‰¹å¾æå–å¤±è´¥: {e}")
            return None
    
    def recognize(self, query_embedding):
        """è¯†åˆ«äººè„¸"""
        if query_embedding is None:
            return "stranger", 0.0
            
        if not self.gallery_embeddings:
            return "stranger", 0.0
        
        best_similarity = 0.0
        best_subject = "stranger"
        
        for subject_id, embeddings in self.gallery_embeddings.items():
            for gallery_embedding in embeddings:
                if gallery_embedding is None:
                    continue
                    
                similarity = np.dot(query_embedding, gallery_embedding)
                
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_subject = subject_id
        
        if best_similarity < self.threshold:
            return "stranger", best_similarity
        else:
            return best_subject, best_similarity
    
    def process_frame(self, frame, trace_id):
        """å¤„ç†å•å¸§å›¾åƒ"""
        # æ£€æµ‹äººè„¸
        faces = self.detect_faces(frame)
        
        # æå–ç‰¹å¾å¹¶è¯†åˆ«
        results = []
        for face in faces:
            feature = self.extract_feature(frame, face['bbox'])
            if feature is None:
                continue
            
            identity, similarity = self.recognize(feature)
            
            results.append({
                'bbox': face['bbox'],
                'features': feature,
                'identity': identity,
                'similarity': float(similarity),
                'score': float(face['score'])
            })
        
        # æ›´æ–°è·Ÿè¸ªå™¨
        tracked_faces = self.tracker.update(results)
        
        # å‡†å¤‡äº‹ä»¶æ•°æ®
        events = []
        for track_id, track_data in tracked_faces.items():
            if len(track_data['features']) > 0:
                latest_features = track_data['features'][-1]
                if latest_features is None:
                    continue
                    
                identity, similarity = self.recognize(latest_features)
                latest_centroid = track_data['centroids'][-1]
                
                event_data = {
                    'ts': time.time(),
                    'trace_id': trace_id,
                    'source': 'vision',
                    'track_id': track_id,
                    'bbox': self._estimate_bbox_from_centroid(latest_centroid, frame.shape),
                    'identity': identity,
                    'similarity': float(similarity),
                    'quality': float(track_data.get('score', 0.5))
                }
                
                events.append(event_data)
        
        return events, results
    
    def _estimate_bbox_from_centroid(self, centroid, image_shape):
        """ä»ä¸­å¿ƒç‚¹ä¼°è®¡è¾¹ç•Œæ¡†"""
        h, w = image_shape[:2]
        cx, cy = centroid
        bbox_size = min(w, h) * 0.2
        
        x1 = max(0, int(cx - bbox_size / 2))
        y1 = max(0, int(cy - bbox_size / 2))
        x2 = min(w, int(cx + bbox_size / 2))
        y2 = min(h, int(cy + bbox_size / 2))
        
        return [x1, y1, x2, y2]
    
    def draw_detections(self, frame, results):
        """åœ¨å›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ"""
        for result in results:
            x1, y1, x2, y2 = result['bbox']
            identity = result['identity']
            similarity = result['similarity']
            
            color = (0, 255, 0) if identity != "stranger" else (0, 0, 255)
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            
            label = f"{identity} ({similarity:.2f})"
            cv2.putText(frame, label, (x1, y1-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)

class RealTimeFaceRecognition:
    """å®æ—¶äººè„¸è¯†åˆ«ç³»ç»Ÿ"""
    
    def __init__(self, model_path, gallery_dir, event_bus=None):
        # ä½¿ç”¨ä¸»ç¨‹åºçš„äº‹ä»¶æ€»çº¿ï¼ˆå¯é€‰ï¼‰
        self.event_bus = event_bus
        
        # æ·»åŠ ç›´æ¥å›è°ƒæ”¯æŒ
        self.direct_callback = None
        
        # åˆå§‹åŒ–äººè„¸è¯†åˆ«å™¨
        self.recognizer = BuffaloFaceRecognizer(model_path, gallery_dir)
        
        # åˆå§‹åŒ–æ‘„åƒå¤´
        self.camera = cv2.VideoCapture(0)
        if not self.camera.isOpened():
            raise Exception("æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
        
        # è®¾ç½®æ‘„åƒå¤´å‚æ•°
        self.camera.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        self.camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        
        # æ§åˆ¶å˜é‡
        self.is_running = False
        self.is_paused = True
        self.processing_thread = None
        
        # ä½¿ç”¨é˜Ÿåˆ—è¿›è¡Œçº¿ç¨‹é—´é€šä¿¡
        self.event_queue = asyncio.Queue()
        self.event_processor_task = None
        
        # ä¿å­˜ä¸»äº‹ä»¶å¾ªç¯çš„å¼•ç”¨
        self.main_loop = None
        
        print("å®æ—¶äººè„¸è¯†åˆ«ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def set_direct_callback(self, callback):
        """è®¾ç½®ç›´æ¥å›è°ƒå‡½æ•°"""
        self.direct_callback = callback
        print("âœ… å·²è®¾ç½®ç›´æ¥å›è°ƒå‡½æ•°")
    
    async def start(self):
        """å¯åŠ¨ç³»ç»Ÿ"""
        if self.is_running:
            print("ç³»ç»Ÿå·²åœ¨è¿è¡Œä¸­")
            return
        
        self.is_running = True
        
        # ä¿å­˜ä¸»äº‹ä»¶å¾ªç¯
        self.main_loop = asyncio.get_running_loop()
        
        # å¯åŠ¨äº‹ä»¶å¤„ç†å™¨ï¼ˆå¦‚æœä½¿ç”¨äº‹ä»¶æ€»çº¿ï¼‰
        if self.event_bus:
            self.event_processor_task = asyncio.create_task(self._event_processor())
        
        # å¯åŠ¨è§†è§‰å¤„ç†çº¿ç¨‹
        self.processing_thread = threading.Thread(target=self._processing_loop)
        self.processing_thread.daemon = True
        self.processing_thread.start()
        
        print("å®æ—¶äººè„¸è¯†åˆ«ç³»ç»Ÿå·²å¯åŠ¨")
    
    async def stop(self):
        """åœæ­¢ç³»ç»Ÿ"""
        self.is_running = False
        
        # åœæ­¢è§†è§‰å¤„ç†çº¿ç¨‹
        if self.processing_thread and self.processing_thread.is_alive():
            self.processing_thread.join(timeout=5.0)
        
        # åœæ­¢äº‹ä»¶å¤„ç†å™¨
        if self.event_processor_task and not self.event_processor_task.done():
            self.event_processor_task.cancel()
            try:
                await self.event_processor_task
            except asyncio.CancelledError:
                pass
        
        self.camera.release()
        cv2.destroyAllWindows()
        
        print("å®æ—¶äººè„¸è¯†åˆ«ç³»ç»Ÿå·²åœæ­¢")
    
    # æ–°å¢ pause å’Œ resume æ–¹æ³•
    def pause(self):
        """æš‚åœè§†è§‰å¤„ç†"""
        if not self.is_paused:
            print("â¸ï¸  è§†è§‰æ¨¡å—è¢«å¤–éƒ¨æš‚åœ")
            self.is_paused = True

    def resume(self):
        """æ¢å¤è§†è§‰å¤„ç†"""
        if self.is_paused:
            print("â–¶ï¸  è§†è§‰æ¨¡å—è¢«å¤–éƒ¨æ¢å¤")
            self.is_paused = False
    
    def _processing_loop(self):
        """å¤„ç†å¾ªç¯ - åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­è¿è¡Œ"""
        frame_interval = 0.2
        last_process_time = 0
        last_triggered_identity = None
        last_trigger_time = 0
        
        print("ğŸ‘ï¸ è§†è§‰å¤„ç†å¾ªç¯å¼€å§‹")
        
        while self.is_running:
            
            # æ–°å¢æš‚åœæ£€æŸ¥é€»è¾‘
            if self.is_paused:
                time.sleep(0.5)
                continue
            
            current_time = time.time()
            
            # æ§åˆ¶å¤„ç†é¢‘ç‡
            if current_time - last_process_time < frame_interval:
                time.sleep(0.01)
                continue
            
            # è¯»å–å¸§
            ret, frame = self.camera.read()
            if not ret:
                print("æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
                time.sleep(1.0)
                continue
            
            # ç”Ÿæˆè¿½è¸ªID
            trace_id = str(uuid.uuid4()).replace('-', '')
            
            # å¤„ç†å¸§
            try:
                events, results = self.recognizer.process_frame(frame, trace_id)
                
                # æ‰“å°è¯†åˆ«ç»“æœ
                if results:
                    print(f"\nå½“å‰å¸§è¯†åˆ«ç»“æœ:")
                    for result in results:
                        print(f"èº«ä»½: {result['identity']}, ç›¸ä¼¼åº¦: {result['similarity']:.3f}")
                
                # å°†äº‹ä»¶æ”¾å…¥é˜Ÿåˆ— - æ·»åŠ å»é‡é€»è¾‘
                for event_data in events:
                    identity = event_data['identity']
                    similarity = event_data['similarity']
                    
                    # è¿‡æ»¤æ¡ä»¶ï¼šç›¸ä¼¼åº¦è¶³å¤Ÿé«˜ï¼Œä¸”ä¸æ˜¯é™Œç”Ÿäººï¼Œä¸”ä¸æ˜¯æœ€è¿‘è§¦å‘è¿‡çš„
                    if (similarity > 0.6 and 
                        identity != "stranger" and 
                        not (identity == last_triggered_identity and 
                            current_time - last_trigger_time < 10.0)):
                        
                        print(f"âœ… å‘é€è¯†åˆ«äº‹ä»¶: {identity} (ç›¸ä¼¼åº¦: {similarity:.3f})")
                        
                        # ä¿®æ­£äº‹ä»¶æ•°æ®æ ¼å¼ä»¥åŒ¹é… router.on_face çš„æœŸæœ›
                        corrected_event_data = {
                            'keyword': identity,
                            'confidence': similarity,
                            'ts': time.time(),
                            'trace_id': trace_id,
                            'source': 'vision'
                        }
                        
                        # ä½¿ç”¨ç›´æ¥å›è°ƒï¼ˆä¼˜å…ˆï¼‰
                        if self.direct_callback:
                            try:
                                print(f"ğŸ¯ ä½¿ç”¨ç›´æ¥å›è°ƒå‘é€: {identity}")
                                self.direct_callback("core.face_id_resolved", corrected_event_data)
                            except Exception as e:
                                print(f"âŒ ç›´æ¥å›è°ƒå¤±è´¥: {e}")
                        
                        # å¦‚æœè®¾ç½®äº†äº‹ä»¶æ€»çº¿ï¼Œä¹Ÿå‘é€åˆ°äº‹ä»¶æ€»çº¿
                        elif self.event_bus and self.main_loop:
                            try:
                                future = asyncio.run_coroutine_threadsafe(
                                    self.event_bus.publish("core.face_id_resolved", corrected_event_data),
                                    self.main_loop
                                )
                                result = future.result(timeout=2.0)
                                print(f"ğŸ¯ äº‹ä»¶æ€»çº¿å‘å¸ƒç»“æœ: {'æˆåŠŸ' if result else 'å¤±è´¥'}")
                            except Exception as e:
                                print(f"âŒ äº‹ä»¶æ€»çº¿å‘å¸ƒå¤±è´¥: {e}")
                        
                        last_triggered_identity = identity
                        last_trigger_time = current_time
                    else:
                        print(f"â­ï¸  è·³è¿‡äº‹ä»¶: {identity} (ç›¸ä¼¼åº¦: {similarity:.3f})")
                
                # åœ¨å¸§ä¸Šç»˜åˆ¶ç»“æœ
                self.recognizer.draw_detections(frame, results)
                
                # æ˜¾ç¤ºå¸§
                cv2.imshow('å®æ—¶äººè„¸è¯†åˆ«', frame)
                
                # æ£€æŸ¥é€€å‡ºé”®
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    self.is_running = False
                
                last_process_time = current_time
                
            except Exception as e:
                print(f"å¤„ç†å¸§æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                time.sleep(1.0)
            # ä»¥ä¸‹æ–¹æ³•å¯ä»¥ä¿ç•™ï¼Œä½†åœ¨è¿™ä¸ªç®€åŒ–ç‰ˆæœ¬ä¸­å¯èƒ½ä¸ä¼šç”¨åˆ°
    async def _event_processor(self):
        """å¤„ç†æ¥è‡ªè§†è§‰çº¿ç¨‹çš„äº‹ä»¶ï¼ˆå¦‚æœä½¿ç”¨äº‹ä»¶æ€»çº¿ï¼‰"""
        if not self.event_bus:
            return
            
        print("ğŸ”„ äº‹ä»¶å¤„ç†å™¨å¯åŠ¨")
        while self.is_running:
            try:
                event_type, event_data = await asyncio.wait_for(
                    self.event_queue.get(), 
                    timeout=1.0
                )
                await self.event_bus.publish(event_type, event_data)
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                print(f"äº‹ä»¶å¤„ç†é”™è¯¯: {e}")
    
    def _put_event_to_queue(self, event_type, event_data):
        """å°†äº‹ä»¶æ”¾å…¥é˜Ÿåˆ—çš„çº¿ç¨‹å®‰å…¨æ–¹æ³•"""
        if self.main_loop and not self.main_loop.is_closed():
            self.main_loop.call_soon_threadsafe(
                lambda: asyncio.create_task(
                    self.event_queue.put((event_type, event_data))
                )
            )
        else:
            print("ä¸»äº‹ä»¶å¾ªç¯ä¸å¯ç”¨ï¼Œæ— æ³•å‘é€äº‹ä»¶")